{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #use for handle requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time #use for set sleep time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup #use for parse html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string #use for punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json #use for write & read json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #use for create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords #use for detect stopwords in string\n",
    "from nltk.tokenize import word_tokenize #use for getting words in string\n",
    "from nltk.stem import PorterStemmer #use for stemmer of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this part is use for getting all movie links in given urls in list\n",
    "urls = [\"https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies1.html\",\n",
    "       \"https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies2.html\",\n",
    "        \"https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies3.html\"\n",
    "       ] #given urls for getting movies urls\n",
    "movies_list = []\n",
    "for url in urls: #for loops for iterate each urls in urls lists\n",
    "    response = requests.get(url) #get url contact\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    for link in soup.find_all(\"a\", href = True): #finding all links and iterate over each one\n",
    "        link_string = str(link) #changinge link datatype to string\n",
    "        first_split = link_string.split(\">\")\n",
    "        for item in first_split: #iterate over first split items\n",
    "            if \"href\" not in item and item != \"\": #use for finding limks in each of split items\n",
    "                first_match = item\n",
    "                second_split = first_match.split(\"<\") #use split for only getting link address\n",
    "                second_match = second_split[0]\n",
    "                movies_list.append(second_match) #append link address to movies list\n",
    "movies_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this part is used to save html files of each movie link\n",
    "for counter_html in range(len(movies_list)): #iterates over item indexes in movies list\n",
    "    movie = movies_list[counter_html] #set link by index\n",
    "    try:\n",
    "        response = requests.get(movie)\n",
    "        text = response.text\n",
    "        file_name_html = \"/Users/vahidmohammadi/™️/DS/Practice/project/03/HTML Files/\" + str(counter_html) + \".html\" #path + file name + file suffix\n",
    "        with open(file_name_html, \"w\") as new_movie_file: #open file by above information for saving each file of movie links\n",
    "            new_movie_file.write(text)\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this part is used to extract our target information from html files and save them in tsv files\n",
    "for counter_tsv in range(len(movies_list)):#use for iterate over each html file\n",
    "    try:\n",
    "        file_name_html = \"/Users/vahidmohammadi/™️/DS/Practice/project/03/HTML Files/\" + str(counter_tsv)+\".html\"#file path of html files\n",
    "        with open(file_name_html, \"r\") as movie_html_file: #open  html files for reading\n",
    "            content = movie_html_file.read() #read html file\n",
    "            soup = BeautifulSoup(content, \"html.parser\") #use beautiful soup for parse html contact\n",
    "            file_name_tsv = \"/Users/vahidmohammadi/™️/DS/Practice/project/03/TSV File/\" + str(counter_tsv)+\".tsv\" #file path of tsv files\n",
    "            with open(file_name_tsv, \"w\") as movie_tsv_file: #open tsv file for writing\n",
    "                title = soup.title.string #get title of html file\n",
    "                intro = soup(\"p\")[0].getText().rstrip(\"\\n\") #get intro of html files\n",
    "                plot = soup(\"p\")[1].getText().rstrip(\"\\n\") #get plot of html files\n",
    "                movie_tsv_file.write(title+\"\\t\"+intro+\"\\t\"+plot+\"\\t\") #writting title, intro and plot in tsv file\n",
    "                table = soup.find_all(\"table\", class_=\"infobox vevent\")[0]\n",
    "                second_soup = BeautifulSoup(str(table), \"html.parser\")\n",
    "                for item in second_soup.strings:\n",
    "                    if item.strip(\"\\n\")!=\"\":\n",
    "                        movie_tsv_file.write(item.rstrip(\"\\n\")+\"\\t\")\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this part is used for removing stopwords punctuation and get the stem words in tsv files and\n",
    "#indexing each new words and create dictionary of words exist on movie's tsv file\n",
    "words = {} #for indexing each word\n",
    "movies_words = {} #for existance of each word in movie's tsv file\n",
    "word_id = 0\n",
    "counter_nltk = 0\n",
    "exclude = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer() #use for finding stem of each word\n",
    "for counter_nltk in range(len(movies_list)):\n",
    "    file_name_tsv = \"/Users/vahidmohammadi/™️/DS/Practice/project/03/TSV File/\" + str(counter_nltk)+\".tsv\"#tsv file path\n",
    "    with open(file_name_tsv, \"r\") as movie_tsv_file:#open tsv files\n",
    "        text_list = movie_tsv_file.read()\n",
    "        text = \" \".join(text_list.split(\"\\t\")[1:3])\n",
    "        text = ''.join(ch for ch in text if ch not in exclude) #use for getting each word without white spaces\n",
    "        word_tokens = word_tokenize(text)\n",
    "        filtered_sentence = [ps.stem(w) for w in word_tokens if not w in stop_words]#removing stopwords and finding stem of the each word\n",
    "        for word in filtered_sentence:\n",
    "            if word not in words.values():#looking for NOT existance of word in words list for indexing\n",
    "                words[word_id] = word\n",
    "                movies_words[word_id] = [counter_nltk]#adding movies name in movies_words\n",
    "                word_id += 1\n",
    "            else: # if word exist in words list just add movies ID (movies file name that produce by counter) in movies word\n",
    "                exist_id = [key for key, value in words.items() if value == word][0]\n",
    "                if counter_nltk not in movies_words[exist_id]:\n",
    "                    movies_words[exist_id].append(counter_nltk)\n",
    "with open(\"words_index.json\", \"w\") as word_index: #write words list in json file\n",
    "    json.dump(words, word_index)\n",
    "with open(\"movies_words.json\", \"w\") as movies_words_file:#write movies_words list in json file\n",
    "    json.dump(movies_words, movies_words_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==============================\")\n",
    "print(\"|What movie to watch tonight?|\")\n",
    "print(\"==============================\")\n",
    "print(\"--------------------\")\n",
    "print(\"|Search Engine v1.0|\")\n",
    "print(\"--------------------\")\n",
    "our_input = input(\"search: \")\n",
    "exclude = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "our_input = ''.join(ch for ch in our_input if ch not in exclude)\n",
    "word_tokens = word_tokenize(our_input)\n",
    "filtered_sentence = [ps.stem(w) for w in word_tokens if not w in stop_words]\n",
    "words_in_input = []\n",
    "with open(\"words_index.json\", \"r\") as word_index:\n",
    "    words = json.load(word_index)\n",
    "with open(\"movies_words.json\", \"r\") as movies_words_file:\n",
    "    movies_words = json.load(movies_words_file)\n",
    "for word in filtered_sentence:\n",
    "    for key, value in words.items():\n",
    "        if word == value:\n",
    "            words_in_input.append(key)\n",
    "words_in_movies = []\n",
    "for key_of_input in words_in_input:#finding each searched word's index\n",
    "    for key_of_movies, value in movies_words.items():\n",
    "        if key_of_input == key_of_movies:\n",
    "            words_in_movies.append(value)\n",
    "final_result_dict = {}#use for saving matches\n",
    "for first_item in words_in_movies:#searching by words index that exist in movies words list\n",
    "    for second_item in first_item:\n",
    "        if second_item in final_result_dict.keys():\n",
    "            final_result_dict[second_item] += 1\n",
    "        else:\n",
    "            final_result_dict[second_item] = 1\n",
    "all_item_match_dict = {}\n",
    "for key, value in final_result_dict.items():\n",
    "    if value == len(words_in_input):\n",
    "        all_item_match_dict[key] = value\n",
    "print(\"\")\n",
    "print(\"Result: \")\n",
    "df = {\"movie\":[], \"intro\":[], \"link\":[]}#use for creating pandas dataframe\n",
    "for item in all_item_match_dict:\n",
    "    file_name = \"/Users/vahidmohammadi/™️/DS/Practice/project/03/TSV File/\" + str(item) + \".tsv\"#tsv file path\n",
    "    with open(file_name, \"r\") as target:#open tsv file for reading title and intro\n",
    "        tsv_file = target.read()\n",
    "        movie_list = tsv_file.split(\"\\t\")\n",
    "        df[\"movie\"].append(movie_list[0])\n",
    "        df[\"intro\"].append(movie_list[1])\n",
    "        df[\"link\"].append(movies_list[item])\n",
    "pd_df = pd.DataFrame(df)\n",
    "if pd_df.empty:\n",
    "    print(\"NOT Found 404\")\n",
    "pd_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
